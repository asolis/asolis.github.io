<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="author" content="Andres Solis Montero">
<link rel="icon" href="favicon.ico">
<title>A General Framework for Fast 3D Object Detection and Localization Using an Uncalibrated Camera</title>
<!-- Bootstrap -->
<link href="css/bootstrap.css" rel="stylesheet">
<link href="css/template.css"  rel="stylesheet">
<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>
<nav class="navbar navbar-fixed-top">
  <div class="container">
    <div class="navbar-header"> <a href="http://www.site.uottawa.ca/research/viva/"> <img  class="navbar-brand"  alt="Brand" src="images/viva-logo.png"> </a> <span  class="navbar-brand brand-text"  alt="Brand" >Research Lab </span> </div>
    <div id="navbar" class="collapse navbar-collapse">
      <ul class="nav navbar-nav">
        <!-- <li class="active"><a href="http://www.site.uottawa.ca/research/viva/">Research Lab</a></li>-->
      </ul>
      <a href="http://engineering.uottawa.ca/eecs/"><img class="navbar-text navbar-right" src="images/uottawa_hor_black.png"/></a> </div>
    
    <!--/.nav-collapse --> 
  </div>
</nav>
<div class="container">
  <div class="starter-template">
    <div class="row">
      <div class="col-lg-7 col-md-7 col-xs-7">
        <h2>A General Framework for Fast 3D Object Detection and Localization Using an Uncalibrated Camera</h2>
        <a  target="newtab" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7045976&punumber%3D7045624%26filter%3DAND(p_IS_Number%3A7045853)%26pageNumber%3D5"><span class="glyphicon glyphicon-link" aria-hidden="true"></span> Applications of Computer Vision (WACV), Waikoloa, HI, USA, January 2015  </a> <br>
        <span class="pull-right small">Posted: Jan 7, 2015 </span>
        <hr>
        
        <h4>Abstract</h4>
        <p>
        We present a real-time approach for 3D object detection using a single, mobile and uncalibrated camera. We develop our algorithm using a feature-based method based on two novel naive Bayes classifiers for viewpoint and feature matching. Our algorithm exploits the specific structure of various binary descriptors in order to boost feature matching by conserving descriptor properties (e.g., rotational and scale invariance, robustness to illumination variations and real-time performance). Unlike state-of-the-art methods, our novel naive classifiers only require a database with a small memory footprint because we store efficiently encoded features. In addition, we also im- prove the indexing scheme to speed up the matching process. Because our database is built from powerful descriptors, only a few images need to be ’learned’ and constructing a database for a new object is highly efficient.

        </p>
        <br>
        <!-- 16:9 aspect ratio -->
<div class="embed-responsive embed-responsive-16by9">
  <iframe class="embed-responsive-item" src="data/3d/3d.mp4"></iframe>
</div>
<span class="small">Media 1. Comparison betwen Ferns and our classifier plus more examples. The mousepad (i.e., single view object) needs around 16mb for the Ferns classifier to learn the model. The same example takes less than 80Kb with our classifier. The learning time for this example using the Ferns classifier takes 348s, our algorithm take 24s for this object. The car toy uses 8 views with a model size of less than 450Kb. The coffee cup uses 6 views with a model size of 340Kb. Each view contains ~200 keypoints.</span>
        <br>
        <br>
        
        <h4>Contributions</h4>
        <p>
        <ul>
        	<li> Real-time 3D object detection using a single mobile and uncalibrated camera</li>
            <li> Combine binary descriptors with Naive Bayes classifiers for feature classification and matching </li>
            <li> Our classifier exploits the specific structure of binary descriptors to increase feature matching while conserving descriptor properties</li>
            <li> Small memory footprint due to efficiently encoded features</li>
            <li> Learning time is reduced because invariant features and descriptors</li>
            <li> Improved indexing scheme to speed up keypoint matching</li>
        </ul>
        </p>
        
        
        <br>
        <h4>Results</h4>
        <p>
        We compare our framework against Ferns and Binary Descriptors in terms of performance, memory usage, and running time. Our framework uses the binary descriptors’ implementations of OpenCV (i.e., BRIEF, ORB, BRISK, FREAK) with their default parameter configurations. The Ferns implementation was the authors’ from their web-page. We use it unmodified except for adapting their training intervals to show the impact of training on their solution and ours. <p>
        <p> First, we compare the memory usage between binary descriptors without a training phase for Ferns and our frame- work. It is easy to see that binary descriptors are the most compact with a memory usage of bits × K, where bits is the size in bits of the descriptor and K the amount of the keypoints in the database. On the other hand, the memory footprint of the Ferns database grows exponentially with the Ferns’size S, i.e., 2S × M × byteS × K, where M is the number of Ferns and byteS is the size of bytes used to store the conditional probabilities. The original Fern implementation uses S = 11, M = 30 and byteS = 4 (float). However, the memory needed for our database is 8 × bits × K, i.e., it is O(K) just as the binary descriptors. We use a byte to represent every bit of the descriptor. For example, 1000 keypoints using the BRIEF descriptor will require 31.25Kb for storage, Ferns will require 234Mb, and ours 250Kb. Or put differently, adding an extra keypoint to the Ferns database will require 240Kb, almost as much as another 1000 points in our representation.</p>
        
        <br><br>
        <p>
        <img class="img-thumbnail" src="data/3d/memory.png"/> <br>
        <span class="small"> Figure 1. Memory usage for 1000 keypoints. Our solution is independent of the patch/tree/fern sizes.</span>
        </p>
        
        <p>
        Next, we evaluate the performance of each algorithm under different images transformations. We synthetically generate perspective transformations of the planar object at different scales and different positions in the image with changes in contrast and brightness. The background is filled with white noise. 
        </p>
        <br><br>
        <p>
        <img class="img-thumbnail" src="data/3d/rates.png"/><br>
        <span class="small"> Table 1. Comparison of detection rates between matching with only binary descriptors, with Ferns and with our Naive Bayes Classified Descriptors (NBCD) when combined with binary descriptors. The detection rates were tested under random rotation, scale and illumination variations and averaged. The detection rates of Ferns are listed with similar training range to ours and in the line Ferns* with complete training. </span>
        </p>
        <br><br>
        <p>
        <img class="img-thumbnail" src="data/3d/mousepad1.png"/>
        <img class="img-thumbnail" src="data/3d/mousepad.png"/> <br>
       <span class="small">	Figure 2. Comparison between Ferns and our framework (i.e., NBCD + ORB) for the mousepad example of Ozuysal et al. We compare speed (i.e., frame per seconds), detection rate, memory usage and construction time of the model.</span>
       </p>
        <br><br>
        <p>
        <img class="img-thumbnail" src="data/3d/speed.png"/> <br>
       <span class="small">	Figure 3. Running time of our framework using different binary descriptors. The running time is separated by keypoint detection, descriptor extraction, keypoint matching, view matching and localization of the object. Oour framework is independent of the keypoint extraction and descriptor selection.</span>
       </p>
       
       <p>
       Our framework detects non-planar 3D objects using the fundamental matrix to capture the geometric constraint between different views of the object. The objects are detected even if they are partially occluded and in different orientations. 
       </p>
        <br><br>
        <p>
        <img class="img-thumbnail" src="data/3d/fundamental.matrix.png"/> <br>
       <span class="small">	Figure 4. The planarity assumption in ferns does not perform well with more complex non planar objects (e.g., top images). The use of the fundamental matrix is stable and can be used with planar and non-planar objects.</span>
       </p>
       
        <br><br>
        <p>
        <img class="img-thumbnail" src="data/3d/classifier.hamming.png"/> <br>
       <span class="small">	Figure 5. Our classifier is capable of creating a better classification that using the Hamming distance similarity measure. The matches in the top view are using Hamming distance (i.e., 118 matches: 92 inliers and 26 outliers). The bottom shows our classifier results with, i.e., 84 matches: 82 inliers and 2 outliers. All matches in both images have the same Hamming distance. </span>
       </p>
       
       
      </div>
      
      <div class="col-lg-1 col-md-1 col-xs-1">
      </div>
      
      <div class="col-lg-4 col-md-4 col-xs-4"> <br>
        <h4>Graphical Abstract</h4>
        <p > <img class="img-thumbnail" src="data/3d/graphical.png"/> </p>
        <br>
        <h4>Authors</h4>
        <p >
        
        <ul >
          <li><a href="http://www.solism.ca/">Andr&eacute;s Sol&iacute;s Montero </a></li>
          <li><a href="http://www.site.uottawa.ca/~jlang/">Jochen Lang</a></li>
          <li><a href="http://www.site.uottawa.ca/~laganier/">Robert Lagani&egrave;re</a></li>
        </ul>
        </p>
        <br>
        <h4>Acknowledgment</h4>
        <p >This research was partly funded by the Natural Sciences and Engineering Research Council of Canada (<a src="http://www.nserc-crsng.gc.ca/index_eng.asp">NSERC</a>) under the Engage program with <a href="http://www.youi.tv"> You.i Labs  </a> as the industrial partner. </p>
        <br>
        <h4>Resources</h4>
        <p >
        
        <ul >
          <li> <a href="data/3d/presentation.pdf">Presentation</a>
            <embed  height="240px" src="data/3d/presentation.pdf" type="application/pdf"></embed>
          </li>
          <li> <a href="data/3d/poster.pdf">Poster</a>
            <embed  height="240px" src="data/3d/poster.pdf" type="application/pdf"></embed>
          </li>
          <li><a href="https://github.com/asolis/detection3D">GitHub Respository</a></li>
        </ul>
        </p>
        
        <br>
        <h4>Bibtex</h4>
        <p >
<pre>@inproceedings{ solisetal2015a,
	title=A General Framework for Fast 3D Object Detection and Localization Using an Uncalibrated Camera,
	author=Andrés Solís Montero, Jochen Lang, Robert Laganierè,
	booktitle=IEEE Winter Conference on Applications of Computer Vision (WACV),
	pages=884-891,
	year=2015
}</pre>
        </p>
        
      </div>
    </div>
  </div>
  <!-- end of column 2--> 
</div>
<!-- end of column 1 --> 
<!-- /.container -->
<footer class="footer">
  <div class="container">
    <p class="text-muted">&copy; 2015 All Rights Reserved.</p>
  </div>
</footer>
<!-- Bootstrap core JavaScript --> 
<!-- Placed at the end of the document so the pages load faster --> 
<!-- jQuery (necessary for Bootstrap's JavaScript plugins) --> 
<script src="js/jquery-1.11.2.min.js"></script> 
<script src="js/bootstrap.js"></script> 
<!-- Include all compiled plugins (below), or include individual files as needed -->
</body>
</html>